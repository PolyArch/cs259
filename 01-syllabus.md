---
layout: page
title: Syllabus
---

### Advanced Computer Architecture (CS251a)
* Instructor: Tony Nowatzki
* Term: Spring 2019
* Textbook: None! 

### Course Objectives

The advancements and overwhelming success of Machine Learning has profoundly affected the future of
computer architecture. Not only is performing learning on big-data the leading application driver for future
architectures, but also machine learning techniques can be used to improve hardware efficiency for a wide
variety of application domains.

This course will explore, from a computer architecture perspective, the principles of hardware/software
codesign for machine learning. One thrust of the course will delve into accelerator, CPU, and GPU enhancements
for ML algorithms, including parallelization techniques. The other thrust of the course will
focus on how machine learning can be used to optimize conventional architectures by dynamically learning
and adapting to program behavior.

Several important specific goals are:

* Develop skills in domain-specialization (reason about how application/domain properties can be exploited with hardware mechanisms) 
* Gain understanding of the current state of the art within acceleration for machine learning, both in academia and in industry.

Also, there are some general goals which hold for any architecture/hardware course:

* Gain intuition and reasoning skills regarding fundamental architecture tradeoffs of hardware design choices (performance/area/power/complexity/generality).
* Understand microarchitecture techniques for extracting parallelism and exploiting locality.
* Learn about evaluation methods, including simulation, analytical modeling, and mechanistic models.  

### Course Components:

Logistically, this course has 4 components.  
* *Online discussion on Piazza:* During this course we will read a number of research papers from
  literature.  We will discuss these on Piazza. See
[review]({{site.baseurl}}/04-discussion/) page for more instructions.

* *In-class discussion:*  You should aim to participate in every in-class
  discussion. I will do my best to track who is participating. Remember, this
  class will be fun and interesting if you make it so!

* Mini-Project (15%) There will be two mini projects which can also be performed in groups:
  - Parallelizing a machine learning kernel using CUDA on our V100 GPU. 
  - Build a ML-accelerator simulator, which is correct and produces accurate performance estimates.

* Leading Class Discussion (15%) Each student (or group) will lead 1 lecture

* *Project:*  Project (40%) Group based research/implementation project with 2-4 students. Please see the project
handout, and feel free to use Piazza to help form groups. You will need to propose a project by the beginning
of the 5th week of class, so please start thinking early.
See [project]({{site.baseurl}}/08-project/) page for more details.

## Grade Breakdown
* 30% In-class online discussion
    - Note you can make up for being quiet in class by participating extensively in online discussion
    - And likewise make up for having little online-discussion by being vocal in class
* 15% Mini-projects
    - 7.5% Each
* 15% Leading Class Discussion
* 40% Project


